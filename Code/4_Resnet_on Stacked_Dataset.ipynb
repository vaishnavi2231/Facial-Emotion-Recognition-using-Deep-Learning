{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["## This code apply Pre_trained Resnet50 Model on below Stacked dataset:\n","1. Stack [Originial + Histogram_Median ]\n","2. Stack [Originial + Sharpened Images ]"],"metadata":{"id":"c1pio3LgG2K2"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"nywscsIGWngn"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torchvision import datasets, transforms, models\n","from torch.utils.data import DataLoader\n","import os"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l-uhLiiHW5sd","outputId":"ce47635c-9405-4881-ae9c-95950cd84fb9","executionInfo":{"status":"ok","timestamp":1745417631736,"user_tz":240,"elapsed":17927,"user":{"displayName":"vaishnavi gawale","userId":"12721493173132899852"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["## Load originial Dataset"],"metadata":{"id":"GnogU7jXeJfZ"}},{"cell_type":"code","source":["import zipfile\n","\n","zip_path = \"/content/drive/MyDrive/UB_Study/CVIP_Project/FERDataset.zip\"\n","extract_to = \"/content/FERDataset\"\n","\n","with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n","    zip_ref.extractall(extract_to)\n","\n","print(\"Dataset unzipped to local Colab storage.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tN7HQormXF4B","outputId":"7f7dc952-8ed4-4284-914b-ca1ec77ce154","executionInfo":{"status":"ok","timestamp":1745417646855,"user_tz":240,"elapsed":6998,"user":{"displayName":"vaishnavi gawale","userId":"12721493173132899852"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Dataset unzipped to local Colab storage.\n"]}]},{"cell_type":"code","source":["import os\n","dir = \"/content/FERDataset\"\n","def count_files(directory):\n","    total_files = 0\n","    no_files = 0\n","    for root, dirs, files in os.walk(directory):\n","        total_files += len(files)\n","        no_files += len(files)\n","        print(root,dirs,no_files)\n","        no_files=0\n","    return total_files\n","\n","print(\"Total files extracted:\", count_files(dir))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ljdpNeTxXLSN","outputId":"4afbc8dc-97af-41e2-c8aa-e81e194344b2","executionInfo":{"status":"ok","timestamp":1745417654379,"user_tz":240,"elapsed":18,"user":{"displayName":"vaishnavi gawale","userId":"12721493173132899852"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/FERDataset ['test', 'train'] 0\n","/content/FERDataset/test ['angry', 'disgust', 'surprise', 'happy', 'fear', 'neutral', 'sad'] 0\n","/content/FERDataset/test/angry [] 958\n","/content/FERDataset/test/disgust [] 111\n","/content/FERDataset/test/surprise [] 831\n","/content/FERDataset/test/happy [] 1774\n","/content/FERDataset/test/fear [] 1024\n","/content/FERDataset/test/neutral [] 1233\n","/content/FERDataset/test/sad [] 1247\n","/content/FERDataset/train ['angry', 'disgust', 'surprise', 'happy', 'fear', 'neutral', 'sad'] 0\n","/content/FERDataset/train/angry [] 3995\n","/content/FERDataset/train/disgust [] 436\n","/content/FERDataset/train/surprise [] 3171\n","/content/FERDataset/train/happy [] 7215\n","/content/FERDataset/train/fear [] 4097\n","/content/FERDataset/train/neutral [] 4965\n","/content/FERDataset/train/sad [] 4830\n","Total files extracted: 35887\n"]}]},{"cell_type":"markdown","source":["## Load Histogram_Median Dataset"],"metadata":{"id":"Sp3z8p4oeNy8"}},{"cell_type":"code","source":["import zipfile\n","\n","zip_path = \"/content/drive/MyDrive/UB_Study/CVIP_Project/Preprocessed_Dataset/Histogram_Median.zip\"\n","extract_to = \"/content/Histogram_Median\"\n","\n","with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n","    zip_ref.extractall(extract_to)\n","\n","print(\"Dataset unzipped to local Colab storage.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BQXEm41nXr11","outputId":"37ff93b2-dedb-4ac1-dbdf-383e753eb04c","executionInfo":{"status":"ok","timestamp":1745417668685,"user_tz":240,"elapsed":5603,"user":{"displayName":"vaishnavi gawale","userId":"12721493173132899852"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Dataset unzipped to local Colab storage.\n"]}]},{"cell_type":"code","source":["import os\n","dir = \"/content/Histogram_Median/Histogram_Median\"\n","def count_files(directory):\n","    total_files = 0\n","    no_files = 0\n","    for root, dirs, files in os.walk(directory):\n","        total_files += len(files)\n","        no_files += len(files)\n","        print(root,dirs,no_files)\n","        no_files=0\n","    return total_files\n","\n","print(\"Total files extracted:\", count_files(dir))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bCRGoBWsX1vw","outputId":"b091f27a-ec30-4a48-fbf4-3e708ab643ab","executionInfo":{"status":"ok","timestamp":1745417682528,"user_tz":240,"elapsed":45,"user":{"displayName":"vaishnavi gawale","userId":"12721493173132899852"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/Histogram_Median/Histogram_Median ['test', 'train'] 0\n","/content/Histogram_Median/Histogram_Median/test ['angry', 'disgust', 'surprise', 'happy', 'fear', 'neutral', 'sad'] 0\n","/content/Histogram_Median/Histogram_Median/test/angry [] 958\n","/content/Histogram_Median/Histogram_Median/test/disgust [] 111\n","/content/Histogram_Median/Histogram_Median/test/surprise [] 831\n","/content/Histogram_Median/Histogram_Median/test/happy [] 1774\n","/content/Histogram_Median/Histogram_Median/test/fear [] 1024\n","/content/Histogram_Median/Histogram_Median/test/neutral [] 1233\n","/content/Histogram_Median/Histogram_Median/test/sad [] 1247\n","/content/Histogram_Median/Histogram_Median/train ['angry', 'disgust', 'surprise', 'happy', 'fear', 'neutral', 'sad'] 0\n","/content/Histogram_Median/Histogram_Median/train/angry [] 3995\n","/content/Histogram_Median/Histogram_Median/train/disgust [] 436\n","/content/Histogram_Median/Histogram_Median/train/surprise [] 3171\n","/content/Histogram_Median/Histogram_Median/train/happy [] 7215\n","/content/Histogram_Median/Histogram_Median/train/fear [] 4097\n","/content/Histogram_Median/Histogram_Median/train/neutral [] 4965\n","/content/Histogram_Median/Histogram_Median/train/sad [] 4830\n","Total files extracted: 35887\n"]}]},{"cell_type":"markdown","source":["## Implementation of Stacking the Images"],"metadata":{"id":"GyordxWzHmzv"}},{"cell_type":"code","source":["from torch.utils.data import Dataset\n","from PIL import Image\n","import os\n","import numpy as np\n","\n","class StackedImageDataset(Dataset):\n","    def __init__(self, orig_root_dir, sharp_root_dir, transform=None):\n","        self.orig_root_dir = orig_root_dir\n","        self.sharp_root_dir = sharp_root_dir\n","        self.transform = transform\n","        self.samples = []\n","\n","        for class_name in os.listdir(orig_root_dir):\n","            orig_class_dir = os.path.join(orig_root_dir, class_name)\n","            sharp_class_dir = os.path.join(sharp_root_dir, class_name)\n","\n","            for fname in os.listdir(orig_class_dir):\n","                orig_path = os.path.join(orig_class_dir, fname)\n","                sharp_path = os.path.join(sharp_class_dir, fname)\n","                if os.path.exists(sharp_path):\n","                    self.samples.append((orig_path, sharp_path, class_name))\n","\n","        self.class_to_idx = {name: idx for idx, name in enumerate(sorted(os.listdir(orig_root_dir)))}\n","\n","    def __len__(self):\n","        return len(self.samples)\n","\n","    def __getitem__(self, idx):\n","        orig_path, sharp_path, label = self.samples[idx]\n","\n","        orig_img = Image.open(orig_path).convert('L')\n","        sharp_img = Image.open(sharp_path).convert('L')\n","\n","        orig = np.array(orig_img)\n","        sharp = np.array(sharp_img)\n","        stacked = np.stack([orig, sharp, (sharp - orig + 128)], axis=-1)\n","        stacked = np.clip(stacked, 0, 255).astype(np.uint8)\n","        stacked_img = Image.fromarray(stacked)\n","\n","        if self.transform:\n","            stacked_img = self.transform(stacked_img)\n","\n","        return stacked_img, self.class_to_idx[label]\n","\n","def train():\n","  for epoch in range(num_epochs):\n","    model.train()\n","    correct_train, total_train, train_loss = 0, 0, 0.0\n","\n","    for images, labels in train_loader:\n","        images, labels = images.to(device), labels.to(device)\n","        optimizer.zero_grad()\n","        outputs = model(images)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        _, preds = torch.max(outputs, 1)\n","        correct_train += (preds == labels).sum().item()\n","        total_train += labels.size(0)\n","        train_loss += loss.item() * images.size(0)\n","\n","    train_acc = correct_train / total_train * 100\n","    print(f\"Epoch {epoch+1}: Train Acc: {train_acc:.2f}%\")\n","\n","\n","def evaluate ():\n","    model.eval()\n","    correct_test, total_test, test_loss = 0, 0, 0.0\n","\n","    with torch.no_grad():\n","        for images, labels in test_loader:\n","            images, labels = images.to(device), labels.to(device)\n","            outputs = model(images)\n","            loss = criterion(outputs, labels)\n","            _, preds = torch.max(outputs, 1)\n","            correct_test += (preds == labels).sum().item()\n","            total_test += labels.size(0)\n","            test_loss += loss.item() * images.size(0)\n","\n","\n","    test_acc = correct_test / total_test * 100\n","    print(f\"\\n Test Accuracy: {test_acc:.2f}%\")\n"],"metadata":{"id":"h-bYlitRX33W"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Applying Resnet50 model on Stack [Originial + Histogram_Median ]"],"metadata":{"id":"gGsbQBVnfU57"}},{"cell_type":"code","source":["import torchvision.transforms as transforms\n","from torch.utils.data import DataLoader\n","\n","transform = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.5]*3, std=[0.5]*3)\n","])\n","\n","train_dataset = StackedImageDataset(\n","    orig_root_dir='/content/FERDataset/train',\n","    sharp_root_dir='/content/Histogram_Median/Histogram_Median/train',\n","    transform=transform\n",")\n","\n","test_dataset = StackedImageDataset(\n","    orig_root_dir='/content/FERDataset/test',\n","    sharp_root_dir='/content/Histogram_Median/Histogram_Median/test',\n","    transform=transform\n",")\n","\n","train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n","test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"],"metadata":{"id":"fMpyEplsX-mP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torchvision.models as models\n","import torch.optim as optim\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","model = models.resnet50(pretrained=True)\n","model.fc = nn.Linear(model.fc.in_features, 7)\n","model = model.to(device)\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=1e-4)\n","num_epochs = 15\n","\n","train_acc_list, test_acc_list, train_loss_list, test_loss_list = [], [], [], []\n"],"metadata":{"id":"OafY0tJZYU5q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pzhTOplXezdy","outputId":"07b37c32-cc50-49df-f84d-bc0fd4e265d9","executionInfo":{"status":"ok","timestamp":1745442787713,"user_tz":240,"elapsed":5062223,"user":{"displayName":"vaishnavi gawale","userId":"12721493173132899852"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1: Train Acc: 52.09%\n","Epoch 2: Train Acc: 67.18%\n","Epoch 3: Train Acc: 78.02%\n","Epoch 4: Train Acc: 88.91%\n","Epoch 5: Train Acc: 94.27%\n","Epoch 6: Train Acc: 95.58%\n","Epoch 7: Train Acc: 96.43%\n","Epoch 8: Train Acc: 96.70%\n","Epoch 9: Train Acc: 96.58%\n","Epoch 10: Train Acc: 97.38%\n","Epoch 11: Train Acc: 97.28%\n","Epoch 12: Train Acc: 97.58%\n","Epoch 13: Train Acc: 97.52%\n","Epoch 14: Train Acc: 97.59%\n","Epoch 15: Train Acc: 97.93%\n"]}]},{"cell_type":"code","source":["evaluate()"],"metadata":{"id":"-DCAT1pee0RO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1745446282771,"user_tz":240,"elapsed":35295,"user":{"displayName":"vaishnavi gawale","userId":"12721493173132899852"}},"outputId":"624708b7-875e-49ae-8362-45cd44f5577c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n"," Test Accuracy: 61.70%\n"]}]},{"cell_type":"code","source":["torch.save(model.state_dict(), 'Resnet50_Stack_Ori_Hist_Median.pth')"],"metadata":{"id":"EoXeo4I_gVUD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Resnet Model on Stacking of original and Histogram_median dataset gave the accuracy of 61.70%"],"metadata":{"id":"qoU50ozqHxpM"}},{"cell_type":"markdown","source":["## Applying Resnet50 on Stack [Originial + Sharpened Images ]"],"metadata":{"id":"IBp03vIqfFeU"}},{"cell_type":"code","source":["import zipfile\n","\n","zip_path = \"/content/drive/MyDrive/UB_Study/CVIP_Project/Preprocessed_Dataset/Sharpened.zip\"\n","extract_to = \"/content/Sharpened\"\n","\n","with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n","    zip_ref.extractall(extract_to)\n","\n","print(\"Dataset unzipped to local Colab storage.\")"],"metadata":{"id":"LAiXLEfzYfpc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1745419411074,"user_tz":240,"elapsed":4903,"user":{"displayName":"vaishnavi gawale","userId":"12721493173132899852"}},"outputId":"eadccfc6-e1e9-4733-b3f5-711358cc84c6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Dataset unzipped to local Colab storage.\n"]}]},{"cell_type":"code","source":["import os\n","dir = \"/content/Sharpened/Sharpened\"\n","def count_files(directory):\n","    total_files = 0\n","    no_files = 0\n","    for root, dirs, files in os.walk(directory):\n","        total_files += len(files)\n","        no_files += len(files)\n","        print(root,dirs,no_files)\n","        no_files=0\n","    return total_files\n","\n","print(\"Total files extracted:\", count_files(dir))"],"metadata":{"id":"1nHJo2MCfjoI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1745419419759,"user_tz":240,"elapsed":40,"user":{"displayName":"vaishnavi gawale","userId":"12721493173132899852"}},"outputId":"09e6b162-083e-440b-c147-e397bbc9c691"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/Sharpened/Sharpened ['test', 'train'] 0\n","/content/Sharpened/Sharpened/test ['angry', 'disgust', 'surprise', 'happy', 'fear', 'neutral', 'sad'] 0\n","/content/Sharpened/Sharpened/test/angry [] 958\n","/content/Sharpened/Sharpened/test/disgust [] 111\n","/content/Sharpened/Sharpened/test/surprise [] 831\n","/content/Sharpened/Sharpened/test/happy [] 1774\n","/content/Sharpened/Sharpened/test/fear [] 1024\n","/content/Sharpened/Sharpened/test/neutral [] 1233\n","/content/Sharpened/Sharpened/test/sad [] 1247\n","/content/Sharpened/Sharpened/train ['angry', 'disgust', 'surprise', 'happy', 'fear', 'neutral', 'sad'] 0\n","/content/Sharpened/Sharpened/train/angry [] 3995\n","/content/Sharpened/Sharpened/train/disgust [] 436\n","/content/Sharpened/Sharpened/train/surprise [] 3171\n","/content/Sharpened/Sharpened/train/happy [] 7215\n","/content/Sharpened/Sharpened/train/fear [] 4097\n","/content/Sharpened/Sharpened/train/neutral [] 4965\n","/content/Sharpened/Sharpened/train/sad [] 4830\n","Total files extracted: 35887\n"]}]},{"cell_type":"code","source":["import torchvision.transforms as transforms\n","from torch.utils.data import DataLoader\n","\n","transform = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.5]*3, std=[0.5]*3)\n","])\n","\n","train_dataset = StackedImageDataset(\n","    orig_root_dir='/content/FERDataset/train',\n","    sharp_root_dir='/content/Sharpened/Sharpened/train',\n","    transform=transform\n",")\n","\n","test_dataset = StackedImageDataset(\n","    orig_root_dir='/content/FERDataset/test',\n","    sharp_root_dir='/content/Sharpened/Sharpened/test',\n","    transform=transform\n",")\n","\n","train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n","test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"],"metadata":{"id":"39ic_or4fqfy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torchvision.models as models\n","import torch.optim as optim\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","model = models.resnet50(pretrained=True)\n","model.fc = nn.Linear(model.fc.in_features, 7)  # 7 emotions in FER\n","model = model.to(device)\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=1e-4)\n","num_epochs = 15\n","\n","train()"],"metadata":{"id":"dnIBOND4f8Z1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1745430704632,"user_tz":240,"elapsed":2136716,"user":{"displayName":"vaishnavi gawale","userId":"12721493173132899852"}},"outputId":"ca7c791e-2a26-417a-9136-9e9925e9ac04"},"execution_count":null,"outputs":[{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n","100%|██████████| 97.8M/97.8M [00:00<00:00, 194MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Train Acc: 57.93%\n","Epoch 2: Train Acc: 71.31%\n","Epoch 3: Train Acc: 81.21%\n","Epoch 4: Train Acc: 89.75%\n","Epoch 5: Train Acc: 94.22%\n","Epoch 6: Train Acc: 95.74%\n","Epoch 7: Train Acc: 96.45%\n","Epoch 8: Train Acc: 97.32%\n","Epoch 9: Train Acc: 96.96%\n","Epoch 10: Train Acc: 96.93%\n","Epoch 11: Train Acc: 97.49%\n","Epoch 12: Train Acc: 97.82%\n","Epoch 13: Train Acc: 97.64%\n","Epoch 14: Train Acc: 97.84%\n","Epoch 15: Train Acc: 98.01%\n"]}]},{"cell_type":"code","source":["evaluate()"],"metadata":{"id":"e-LMYFlRgAo0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1745431223758,"user_tz":240,"elapsed":36491,"user":{"displayName":"vaishnavi gawale","userId":"12721493173132899852"}},"outputId":"7bce78e1-0c53-4d29-9244-165a7f63997d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n"," Test Accuracy: 64.64%\n"]}]},{"cell_type":"code","source":["torch.save(model.state_dict(), 'Resnet50_Stack_Ori_Sharpen.pth')"],"metadata":{"id":"eWiijhdhgciM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Resnet50 model on Original + Sharpened dataset gave the highest accuracy among the filtered stacked images."],"metadata":{"id":"7uxEWZG4JYHP"}},{"cell_type":"code","source":[],"metadata":{"id":"olXc-ATC0T9j"},"execution_count":null,"outputs":[]}]}